{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Cookbook\n",
    "\n",
    "### Tomasz Drabas, Denny Lee\n",
    "#### Version: 0.1\n",
    "#### Date: 3/10/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as func\n",
    "forest_path = '../data/forest_coverage_type.csv'\n",
    "\n",
    "forest = spark.read.csv(\n",
    "    forest_path\n",
    "    , header=True\n",
    "    , inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Elevation: integer (nullable = true)\n",
      " |-- Aspect: integer (nullable = true)\n",
      " |-- Slope: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n",
      " |-- Hillshade_9am: integer (nullable = true)\n",
      " |-- Hillshade_Noon: integer (nullable = true)\n",
      " |-- Hillshade_3pm: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n",
      " |-- Wilderness_Area_Rawah: integer (nullable = true)\n",
      " |-- Wilderness_Area_Neota: integer (nullable = true)\n",
      " |-- Wilderness_Area_Comanche: integer (nullable = true)\n",
      " |-- Wilderness_Area_CacheLaPoudre: integer (nullable = true)\n",
      " |-- Soil_type_2702: integer (nullable = true)\n",
      " |-- Soil_type_2703: integer (nullable = true)\n",
      " |-- Soil_type_2704: integer (nullable = true)\n",
      " |-- Soil_type_2705: integer (nullable = true)\n",
      " |-- Soil_type_2706: integer (nullable = true)\n",
      " |-- Soil_type_2717: integer (nullable = true)\n",
      " |-- Soil_type_3501: integer (nullable = true)\n",
      " |-- Soil_type_3502: integer (nullable = true)\n",
      " |-- Soil_type_4201: integer (nullable = true)\n",
      " |-- Soil_type_4703: integer (nullable = true)\n",
      " |-- Soil_type_4704: integer (nullable = true)\n",
      " |-- Soil_type_4744: integer (nullable = true)\n",
      " |-- Soil_type_4758: integer (nullable = true)\n",
      " |-- Soil_type_5101: integer (nullable = true)\n",
      " |-- Soil_type_5151: integer (nullable = true)\n",
      " |-- Soil_type_6101: integer (nullable = true)\n",
      " |-- Soil_type_6102: integer (nullable = true)\n",
      " |-- Soil_type_6731: integer (nullable = true)\n",
      " |-- Soil_type_7101: integer (nullable = true)\n",
      " |-- Soil_type_7102: integer (nullable = true)\n",
      " |-- Soil_type_7103: integer (nullable = true)\n",
      " |-- Soil_type_7201: integer (nullable = true)\n",
      " |-- Soil_type_7202: integer (nullable = true)\n",
      " |-- Soil_type_7700: integer (nullable = true)\n",
      " |-- Soil_type_7701: integer (nullable = true)\n",
      " |-- Soil_type_7702: integer (nullable = true)\n",
      " |-- Soil_type_7709: integer (nullable = true)\n",
      " |-- Soil_type_7710: integer (nullable = true)\n",
      " |-- Soil_type_7745: integer (nullable = true)\n",
      " |-- Soil_type_7746: integer (nullable = true)\n",
      " |-- Soil_type_7755: integer (nullable = true)\n",
      " |-- Soil_type_7756: integer (nullable = true)\n",
      " |-- Soil_type_7757: integer (nullable = true)\n",
      " |-- Soil_type_7790: integer (nullable = true)\n",
      " |-- Soil_type_8703: integer (nullable = true)\n",
      " |-- Soil_type_8707: integer (nullable = true)\n",
      " |-- Soil_type_8708: integer (nullable = true)\n",
      " |-- Soil_type_8771: integer (nullable = true)\n",
      " |-- Soil_type_8772: integer (nullable = true)\n",
      " |-- Soil_type_8776: integer (nullable = true)\n",
      " |-- CoverType: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "forest.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of most popular **Transformers**\n",
    "* Binarizer\n",
    "* Bucketizer\n",
    "* ChiSqSelector\n",
    "* CountVectorizer\n",
    "* DCT\n",
    "* ElementwiseProduct\n",
    "* HashingTF\n",
    "* IDF\n",
    "* IndexToString\n",
    "* MaxAbsScaler\n",
    "* MinMaxScaler\n",
    "* NGram\n",
    "* Normalizer\n",
    "* OneHotEncoder\n",
    "* PCA\n",
    "* PolynomialExpansion\n",
    "* QuantileDiscretizer\n",
    "* RegexTokenizer\n",
    "* RFormula\n",
    "* SQLTransformer\n",
    "* StandardScaler\n",
    "* StopWordsRemover\n",
    "* StringIndexer\n",
    "* Tokenizer\n",
    "* VectorAssembler\n",
    "* VectorIndexer\n",
    "* VectorSlicer\n",
    "* Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+------------------------------------+\n",
      "|Horizontal_Distance_To_Hydrology|Horizontal_Distance_To_Hydrology_Bkt|\n",
      "+--------------------------------+------------------------------------+\n",
      "|                             258|                                 2.0|\n",
      "|                             212|                                 1.0|\n",
      "|                             268|                                 2.0|\n",
      "|                             242|                                 1.0|\n",
      "|                             153|                                 1.0|\n",
      "+--------------------------------+------------------------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.feature as feat\n",
    "import pyspark.sql.functions as f\n",
    "import numpy as np\n",
    "\n",
    "buckets_no = 10\n",
    "\n",
    "dist_min_max = (\n",
    "    forest.agg(\n",
    "        f.min('Horizontal_Distance_To_Hydrology').alias('min'), \n",
    "        f.max('Horizontal_Distance_To_Hydrology').alias('max')\n",
    "    )\n",
    "    .rdd\n",
    "    .map(lambda row: (row.min, row.max))\n",
    "    .collect()[0]\n",
    ")\n",
    "\n",
    "rng = dist_min_max[1] - dist_min_max[0]\n",
    "\n",
    "splits = list(np.arange(dist_min_max[0], dist_min_max[1], rng / (buckets_no + 1)))\n",
    "\n",
    "bucketizer = feat.Bucketizer(\n",
    "    splits=splits\n",
    "    , inputCol='Horizontal_Distance_To_Hydrology'\n",
    "    , outputCol='Horizontal_Distance_To_Hydrology_Bkt'\n",
    ")\n",
    "\n",
    "bucketizer.transform(forest).select('Horizontal_Distance_To_Hydrology','Horizontal_Distance_To_Hydrology_Bkt').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'Field \"feat\" does not exist.'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 105, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 281, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 79, in deco\n",
      "    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.IllegalArgumentException: 'Field \"feat\" does not exist.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorAssembler = feat.VectorAssembler(inputCols=forest.columns, outputCol='feat')\n",
    "pca = feat.PCA(k=5, inputCol=vectorAssembler.getOutputCol(), outputCol='pca_feat')\n",
    "pca.fit(vectorAssembler.transform(forest)).transform(forest).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of most popular **Estimators**\n",
    "1. Classification\n",
    " * LinearSVC\n",
    " * LogisticRegression \n",
    " * DecisionTreeClassifier\n",
    " * GBTClassifier\n",
    " * RandomForestClassifier\n",
    " * NaiveBayes\n",
    " * MultilayerPerceptronClassifier\n",
    " * OneVsRest\n",
    "2. Regression\n",
    " * AFTSurvivalRegression\n",
    " * DecisionTreeRegressor\n",
    " * GBTRegressor\n",
    " * GeneralizedLinearRegression\n",
    " * IsotonicRegression\n",
    " * LinearRegression\n",
    " * RandomForestRegressor\n",
    "3. Clustering\n",
    " * BisectingKMeans\n",
    " * Kmeans\n",
    " * GaussianMixture\n",
    " * LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|CoverType| count|\n",
      "+---------+------+\n",
      "|        1|211840|\n",
      "|        6| 17367|\n",
      "|        3| 35754|\n",
      "|        5|  9493|\n",
      "|        4|  2747|\n",
      "|        7| 20510|\n",
      "|        2|283301|\n",
      "+---------+------+"
     ]
    }
   ],
   "source": [
    "forest.select('CoverType').groupBy('CoverType').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseVector([-0.0001, -0.0, -0.0023, -0.0, -0.0001, 0.0, -0.001, -0.0017, -0.0003, -0.0, 0.0, 0.0401, -0.0071, -0.0958, -0.0901, -0.0653, -0.0655, -0.0437, -0.0928, -0.0848, -0.0211, -0.0045, -0.0498, -0.0829, -0.0522, -0.0325, -0.0263, -0.0923, -0.0889, -0.0275, -0.0606, -0.0595, 0.0341, -0.003, 0.0822, 0.0607, 0.0351, 0.0093, 0.0048, -0.0154, 0.0422, -0.0673, -0.0039, -0.0142, 0.0036, 0.0078, 0.0, -0.0117, 0.0283, -0.0002, -0.0463, 0.0394, 0.0292, 0.0358])"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.classification as cl\n",
    "\n",
    "vectorAssembler = feat.VectorAssembler(inputCols=forest.columns[0:-1], outputCol='features')\n",
    "\n",
    "fir_dataset = (\n",
    "    vectorAssembler\n",
    "    .transform(forest)\n",
    "    .withColumn('label', (f.col('CoverType') == 1).cast('integer'))\n",
    "    .select('label', 'features')\n",
    ")\n",
    "\n",
    "svc_obj = cl.LinearSVC(maxIter=10, regParam=0.01)\n",
    "svc_model = svc_obj.fit(fir_dataset)\n",
    "\n",
    "svc_model.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseVector([0.0309, 0.6522, 0.1911, 0.1424, 0.0342, 0.7402, 1.053, -0.0017, -0.0041, 2.7163, 189.0362, 27.8238, -265.8505, -407.4379, -346.0612, -364.3841, -302.6788, -400.5852, -212.9918, -126.1329, -117.7423, -312.0478, -248.7118, -221.4788, -155.1459, -84.5129, -398.0433, -387.8102, -179.4485, -261.3875, -337.7875, 48.0629, -94.7813, 149.8043, 135.144, 80.0901, 64.3659, 124.0233, -115.0126, 119.1285, -181.7498, 10.8056, -42.7849, 65.5441, 102.2562, 36.9865, -48.1163, 379.2091, 256.0169, 497.1714, 313.0607, 337.172, 397.0758, -14.4551])"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.regression as rg\n",
    "\n",
    "vectorAssembler = feat.VectorAssembler(inputCols=forest.columns[1:], outputCol='features')\n",
    "\n",
    "elevation_dataset = (\n",
    "    vectorAssembler\n",
    "    .transform(forest)\n",
    "    .withColumn('label', f.col('Elevation').cast('float'))\n",
    "    .select('label', 'features')\n",
    ")\n",
    "    \n",
    "lr_obj = rg.LinearRegression(maxIter=10, regParam=0.01, elasticNetParam=1.00)\n",
    "lr_model = lr_obj.fit(elevation_dataset)\n",
    "\n",
    "lr_model.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7860412464754236, 129.50871925702438, 103.34079732698483)"
     ]
    }
   ],
   "source": [
    "summary = lr_model.summary\n",
    "\n",
    "summary.r2, summary.rootMeanSquaredError, summary.meanAbsoluteError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|Elevation|        prediction|\n",
      "+---------+------------------+\n",
      "|     2596|2840.7801831411316|\n",
      "|     2590|2828.7464246669683|\n",
      "|     2804| 2842.761272955131|\n",
      "|     2785| 2966.057500325109|\n",
      "|     2595|2817.1687155114637|\n",
      "+---------+------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "vectorAssembler = feat.VectorAssembler(inputCols=forest.columns[1:], outputCol='features')\n",
    "lr_obj = rg.GeneralizedLinearRegression(\n",
    "    labelCol='Elevation'\n",
    "    , maxIter=10\n",
    "    , regParam=0.01\n",
    "    , link='identity'\n",
    "    , linkPredictionCol=\"p\"\n",
    ")\n",
    "\n",
    "pip = Pipeline(stages=[vectorAssembler, lr_obj])\n",
    "\n",
    "pip.fit(forest).transform(forest).select('Elevation', 'prediction').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting the most predictable features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            selected|\n",
      "+--------------------+\n",
      "|(10,[0,1,2,3,5,6,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "vectorAssembler = feat.VectorAssembler(inputCols=forest.columns[0:-1], outputCol='features')\n",
    "selector = feat.ChiSqSelector(labelCol='CoverType', numTopFeatures=10, outputCol='selected')\n",
    "selector.fit(vectorAssembler.transform(forest)).transform(vectorAssembler.transform(forest)).select('selected').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting forest coverage type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_train, forest_test = forest.randomSplit([0.7, 0.3], seed=666)\n",
    "\n",
    "vectorAssembler = feat.VectorAssembler(inputCols=forest.columns[0:-1], outputCol='features')\n",
    "selector = feat.ChiSqSelector(labelCol='CoverType', numTopFeatures=10, outputCol='selected')\n",
    "logReg_obj = cl.LogisticRegression(\n",
    "    labelCol='CoverType'\n",
    "    , featuresCol=selector.getOutputCol()\n",
    "    , regParam=0.01\n",
    "    , elasticNetParam=1.0\n",
    "    , family='multinomial'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorAssembler, selector, logReg_obj])\n",
    "pModel = pipeline.fit(forest_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+----------+\n",
      "|CoverType|         probability|prediction|\n",
      "+---------+--------------------+----------+\n",
      "|        3|[8.67479362381023...|       3.0|\n",
      "|        3|[9.19887826242121...|       3.0|\n",
      "|        6|[9.00292703965869...|       3.0|\n",
      "|        6|[1.06415417198863...|       3.0|\n",
      "|        6|[1.04354579793880...|       3.0|\n",
      "+---------+--------------------+----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "results_logReg = pModel.transform(forest_test).select('CoverType', 'probability', 'prediction')\n",
    "results_logReg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6007121761095855, 0.590858268546363, 0.6343096137892893)"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.evaluation as ev\n",
    "\n",
    "evaluator = ev.MulticlassClassificationEvaluator(\n",
    "    predictionCol='prediction'\n",
    "    , labelCol='CoverType')\n",
    "\n",
    "(\n",
    "    evaluator.evaluate(results_logReg)\n",
    "    , evaluator.evaluate(results_logReg, {evaluator.metricName: 'weightedPrecision'})\n",
    "    , evaluator.evaluate(results_logReg, {evaluator.metricName: 'accuracy'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_obj = cl.RandomForestClassifier(\n",
    "    labelCol='CoverType'\n",
    "    , featuresCol=selector.getOutputCol()\n",
    "    , minInstancesPerNode=10\n",
    "    , numTrees=10\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorAssembler, selector, rf_obj])\n",
    "pModel = pipeline.fit(forest_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+----------+\n",
      "|CoverType|         probability|prediction|\n",
      "+---------+--------------------+----------+\n",
      "|        3|[0.0,0.0180630802...|       3.0|\n",
      "|        3|[0.0,0.0180630802...|       3.0|\n",
      "|        6|[0.0,0.0180630802...|       3.0|\n",
      "|        6|[0.0,0.0077261652...|       3.0|\n",
      "|        6|[0.0,0.0077261652...|       3.0|\n",
      "+---------+--------------------+----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "results_rf = pModel.transform(forest_test).select('CoverType', 'probability', 'prediction')\n",
    "results_rf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    evaluator.evaluate(results_rf)\n",
    "    , evaluator.evaluate(results_rf, {evaluator.metricName: 'weightedPrecision'})\n",
    "    , evaluator.evaluate(results_rf, {evaluator.metricName: 'accuracy'})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating forest elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering forest cover type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating performance characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretizing continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizing continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
