{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Cookbook\n",
    "\n",
    "### Tomasz Drabas, Denny Lee\n",
    "#### Version: 0.1\n",
    "#### Date: 2/28/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "32561"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as func\n",
    "census_path = '../data/census_income.csv'\n",
    "\n",
    "census = spark.read.csv(\n",
    "    census_path\n",
    "    , header=True\n",
    "    , inferSchema=True\n",
    ")\n",
    "\n",
    "for col, typ in census.dtypes:\n",
    "    if typ == 'string':\n",
    "        census = census.withColumn(\n",
    "            col\n",
    "            , func.ltrim(func.rtrim(census[col]))\n",
    "        )\n",
    "census.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+------+------------+-------------+--------------------+-----------------+-------------+------------------+------+------------+------------+--------------+--------------+-----+\n",
      "|age|       workclass|fnlwgt|   education|education-num|      marital-status|       occupation| relationship|              race|   sex|capital-gain|capital-loss|hours-per-week|native-country|label|\n",
      "+---+----------------+------+------------+-------------+--------------------+-----------------+-------------+------------------+------+------------+------------+--------------+--------------+-----+\n",
      "| 39|       State-gov| 77516|   Bachelors|           13|       Never-married|     Adm-clerical|Not-in-family|             White|  Male|        2174|           0|            40| United-States|<=50K|\n",
      "| 50|Self-emp-not-inc| 83311|   Bachelors|           13|  Married-civ-spouse|  Exec-managerial|      Husband|             White|  Male|           0|           0|            13| United-States|<=50K|\n",
      "| 38|         Private|215646|     HS-grad|            9|            Divorced|Handlers-cleaners|Not-in-family|             White|  Male|           0|           0|            40| United-States|<=50K|\n",
      "| 53|         Private|234721|        11th|            7|  Married-civ-spouse|Handlers-cleaners|      Husband|             Black|  Male|           0|           0|            40| United-States|<=50K|\n",
      "| 28|         Private|338409|   Bachelors|           13|  Married-civ-spouse|   Prof-specialty|         Wife|             Black|Female|           0|           0|            40|          Cuba|<=50K|\n",
      "| 37|         Private|284582|     Masters|           14|  Married-civ-spouse|  Exec-managerial|         Wife|             White|Female|           0|           0|            40| United-States|<=50K|\n",
      "| 49|         Private|160187|         9th|            5|Married-spouse-ab...|    Other-service|Not-in-family|             Black|Female|           0|           0|            16|       Jamaica|<=50K|\n",
      "| 52|Self-emp-not-inc|209642|     HS-grad|            9|  Married-civ-spouse|  Exec-managerial|      Husband|             White|  Male|           0|           0|            45| United-States| >50K|\n",
      "| 31|         Private| 45781|     Masters|           14|       Never-married|   Prof-specialty|Not-in-family|             White|Female|       14084|           0|            50| United-States| >50K|\n",
      "| 42|         Private|159449|   Bachelors|           13|  Married-civ-spouse|  Exec-managerial|      Husband|             White|  Male|        5178|           0|            40| United-States| >50K|\n",
      "| 37|         Private|280464|Some-college|           10|  Married-civ-spouse|  Exec-managerial|      Husband|             Black|  Male|           0|           0|            80| United-States| >50K|\n",
      "| 30|       State-gov|141297|   Bachelors|           13|  Married-civ-spouse|   Prof-specialty|      Husband|Asian-Pac-Islander|  Male|           0|           0|            40|         India| >50K|\n",
      "| 23|         Private|122272|   Bachelors|           13|       Never-married|     Adm-clerical|    Own-child|             White|Female|           0|           0|            30| United-States|<=50K|\n",
      "| 32|         Private|205019|  Assoc-acdm|           12|       Never-married|            Sales|Not-in-family|             Black|  Male|           0|           0|            50| United-States|<=50K|\n",
      "| 40|         Private|121772|   Assoc-voc|           11|  Married-civ-spouse|     Craft-repair|      Husband|Asian-Pac-Islander|  Male|           0|           0|            40|             ?| >50K|\n",
      "| 34|         Private|245487|     7th-8th|            4|  Married-civ-spouse| Transport-moving|      Husband|Amer-Indian-Eskimo|  Male|           0|           0|            45|        Mexico|<=50K|\n",
      "| 25|Self-emp-not-inc|176756|     HS-grad|            9|       Never-married|  Farming-fishing|    Own-child|             White|  Male|           0|           0|            35| United-States|<=50K|\n",
      "| 32|         Private|186824|     HS-grad|            9|       Never-married|Machine-op-inspct|    Unmarried|             White|  Male|           0|           0|            40| United-States|<=50K|\n",
      "| 38|         Private| 28887|        11th|            7|  Married-civ-spouse|            Sales|      Husband|             White|  Male|           0|           0|            50| United-States|<=50K|\n",
      "| 43|Self-emp-not-inc|292175|     Masters|           14|            Divorced|  Exec-managerial|    Unmarried|             White|Female|           0|           0|            45| United-States| >50K|\n",
      "+---+----------------+------+------------+-------------+--------------------+-----------------+-------------+------------------+------+------------+------------+--------------+--------------+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "census.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: integer (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education-num: integer (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital-gain: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- hours-per-week: integer (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- label: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "census.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of columns to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', 'age', 'capital-gain', 'capital-loss', 'hours-per-week', 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']"
     ]
    }
   ],
   "source": [
    "cols_to_keep = census.dtypes\n",
    "\n",
    "cols_to_keep = (\n",
    "    ['label','age'\n",
    "     ,'capital-gain'\n",
    "     ,'capital-loss'\n",
    "     ,'hours-per-week'\n",
    "    ] + [\n",
    "        e[0] for e in cols_to_keep[:-1] \n",
    "        if e[1] == 'string'\n",
    "    ]\n",
    ")\n",
    "\n",
    "cols_to_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get numeric and categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['age', 'capital-gain', 'capital-loss', 'hours-per-week'], ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country'])"
     ]
    }
   ],
   "source": [
    "import pyspark.mllib.stat as st\n",
    "import numpy as np\n",
    "\n",
    "census_subset = census.select(cols_to_keep)\n",
    "\n",
    "cols_num = [\n",
    "    e[0] for e in census_subset.dtypes \n",
    "    if e[1] == 'int'\n",
    "]\n",
    "cols_cat = [\n",
    "    e[0] for e in census_subset.dtypes[1:] \n",
    "    if e[1] == 'string'\n",
    "]\n",
    "cols_num, cols_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age: min->17.0, mean->38.6, max->90.0, stdev->13.6\n",
      "capital-gain: min->0.0, mean->1077.6, max->99999.0, stdev->7385.3\n",
      "capital-loss: min->0.0, mean->87.3, max->4356.0, stdev->403.0\n",
      "hours-per-week: min->1.0, mean->40.4, max->99.0, stdev->12.3"
     ]
    }
   ],
   "source": [
    "rdd_num = (\n",
    "    census_subset\n",
    "    .select(cols_num)\n",
    "    .rdd\n",
    "    .map(lambda row: [e for e in row])\n",
    ")\n",
    "\n",
    "stats_num = st.Statistics.colStats(rdd_num)\n",
    "\n",
    "for col, min_, mean_, max_, var_ in zip(\n",
    "      cols_num\n",
    "    , stats_num.min()\n",
    "    , stats_num.mean()\n",
    "    , stats_num.max()\n",
    "    , stats_num.variance()\n",
    "):\n",
    "    print('{0}: min->{1:.1f}, mean->{2:.1f}, max->{3:.1f}, stdev->{4:.1f}'\n",
    "          .format(col, min_, mean_, max_, np.sqrt(var_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex [('Male', 21790), ('Female', 10771)] \n",
      "\n",
      "race [('White', 27816), ('Black', 3124), ('Asian-Pac-Islander', 1039), ('Amer-Indian-Eskimo', 311), ('Other', 271)] \n",
      "\n",
      "label [('<=50K', 24720), ('>50K', 7841)] \n",
      "\n",
      "native-country [('United-States', 29170), ('Mexico', 643), ('?', 583), ('Philippines', 198), ('Germany', 137), ('Canada', 121), ('Puerto-Rico', 114), ('El-Salvador', 106), ('India', 100), ('Cuba', 95), ('England', 90), ('Jamaica', 81), ('South', 80), ('China', 75), ('Italy', 73), ('Dominican-Republic', 70), ('Vietnam', 67), ('Guatemala', 64), ('Japan', 62), ('Poland', 60), ('Columbia', 59), ('Taiwan', 51), ('Haiti', 44), ('Iran', 43), ('Portugal', 37), ('Nicaragua', 34), ('Peru', 31), ('France', 29), ('Greece', 29), ('Ecuador', 28), ('Ireland', 24), ('Hong', 20), ('Trinadad&Tobago', 19), ('Cambodia', 19), ('Laos', 18), ('Thailand', 18), ('Yugoslavia', 16), ('Outlying-US(Guam-USVI-etc)', 14), ('Hungary', 13), ('Honduras', 13), ('Scotland', 12), ('Holand-Netherlands', 1)] \n",
      "\n",
      "marital-status [('Married-civ-spouse', 14976), ('Never-married', 10683), ('Divorced', 4443), ('Separated', 1025), ('Widowed', 993), ('Married-spouse-absent', 418), ('Married-AF-spouse', 23)] \n",
      "\n",
      "workclass [('Private', 22696), ('Self-emp-not-inc', 2541), ('Local-gov', 2093), ('?', 1836), ('State-gov', 1298), ('Self-emp-inc', 1116), ('Federal-gov', 960), ('Without-pay', 14), ('Never-worked', 7)] \n",
      "\n",
      "education [('HS-grad', 10501), ('Some-college', 7291), ('Bachelors', 5355), ('Masters', 1723), ('Assoc-voc', 1382), ('11th', 1175), ('Assoc-acdm', 1067), ('10th', 933), ('7th-8th', 646), ('Prof-school', 576), ('9th', 514), ('12th', 433), ('Doctorate', 413), ('5th-6th', 333), ('1st-4th', 168), ('Preschool', 51)] \n",
      "\n",
      "occupation [('Prof-specialty', 4140), ('Craft-repair', 4099), ('Exec-managerial', 4066), ('Adm-clerical', 3770), ('Sales', 3650), ('Other-service', 3295), ('Machine-op-inspct', 2002), ('?', 1843), ('Transport-moving', 1597), ('Handlers-cleaners', 1370), ('Farming-fishing', 994), ('Tech-support', 928), ('Protective-serv', 649), ('Priv-house-serv', 149), ('Armed-Forces', 9)] \n",
      "\n",
      "relationship [('Husband', 13193), ('Not-in-family', 8305), ('Own-child', 5068), ('Unmarried', 3446), ('Wife', 1568), ('Other-relative', 981)]"
     ]
    }
   ],
   "source": [
    "rdd_cat = (\n",
    "    census_subset\n",
    "    .select(cols_cat + ['label'])\n",
    "    .rdd\n",
    "    .map(lambda row: [e for e in row])\n",
    ")\n",
    "\n",
    "results_cat = {}\n",
    "\n",
    "for i, col in enumerate(cols_cat + ['label']):\n",
    "    results_cat[col] = (\n",
    "        rdd_cat\n",
    "        .groupBy(lambda row: row[i])\n",
    "        .map(lambda el: (el[0], len(el[1])))\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "for k in results_cat:\n",
    "    print(\n",
    "        k\n",
    "        , sorted(\n",
    "            results_cat[k]\n",
    "            , key=lambda el: el[1]\n",
    "            , reverse=True)\n",
    "        , '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 1.        ,  0.0776745 ,  0.05777454,  0.06875571],\n",
      "       [ 0.0776745 ,  1.        , -0.03161506,  0.07840862],\n",
      "       [ 0.05777454, -0.03161506,  1.        ,  0.05425636],\n",
      "       [ 0.06875571,  0.07840862,  0.05425636,  1.        ]])"
     ]
    }
   ],
   "source": [
    "correlations = st.Statistics.corr(rdd_num)\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "     capital-gain 0.077674498166\n",
      "     capital-loss 0.057774539479\n",
      "     hours-per-week 0.0687557075095\n",
      "\n",
      "capital-gain\n",
      "     age 0.077674498166\n",
      "     hours-per-week 0.0784086153901\n",
      "\n",
      "capital-loss\n",
      "     age 0.057774539479\n",
      "     hours-per-week 0.0542563622727\n",
      "\n",
      "hours-per-week\n",
      "     age 0.0687557075095\n",
      "     capital-gain 0.0784086153901\n",
      "     capital-loss 0.0542563622727"
     ]
    }
   ],
   "source": [
    "for i, el_i in enumerate(abs(correlations) > 0.05):\n",
    "    print(cols_num[i])\n",
    "    \n",
    "    for j, el_j in enumerate(el_i):\n",
    "        if el_j and j != i:\n",
    "            print(\n",
    "                '    '\n",
    "                , cols_num[j]\n",
    "                , correlations[i][j]\n",
    "            )\n",
    "            \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "the occurrence of the outcomes is statistically independent."
     ]
    }
   ],
   "source": [
    "import pyspark.mllib.linalg as ln\n",
    "\n",
    "census_occupation = (\n",
    "    census\n",
    "    .groupby('occupation')\n",
    "    .pivot('label')\n",
    "    .count()\n",
    ")\n",
    "\n",
    "census_occupation_coll = (\n",
    "    census_occupation\n",
    "    .rdd\n",
    "    .map(lambda row: (row[1:]))\n",
    "    .flatMap(lambda row: row)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "len_row = len(census_occupation.collect())\n",
    "dense_mat = ln.DenseMatrix(\n",
    "    len_row\n",
    "    , 2\n",
    "    , census_occupation_coll\n",
    "    , True\n",
    ")\n",
    "\n",
    "chi_sq = st.Statistics.chiSqTest(dense_mat)\n",
    "\n",
    "print(chi_sq.pValue)\n",
    "print(chi_sq.nullHypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[  2.66700000e+03,   9.83000000e+02],\n",
      "       [  2.09800000e+03,   1.96800000e+03],\n",
      "       [  2.28100000e+03,   1.85900000e+03],\n",
      "       [  1.28400000e+03,   8.60000000e+01],\n",
      "       [  8.79000000e+02,   1.15000000e+02],\n",
      "       [  3.17000000e+03,   9.29000000e+02],\n",
      "       [  1.27700000e+03,   3.20000000e+02],\n",
      "       [  1.48000000e+02,   1.00000000e+00],\n",
      "       [  4.38000000e+02,   2.11000000e+02],\n",
      "       [  3.15800000e+03,   1.37000000e+02],\n",
      "       [  6.45000000e+02,   2.83000000e+02],\n",
      "       [  1.75200000e+03,   2.50000000e+02],\n",
      "       [  8.00000000e+00,   1.00000000e+00],\n",
      "       [  1.65200000e+03,   1.91000000e+02],\n",
      "       [  3.26300000e+03,   5.07000000e+02]])"
     ]
    }
   ],
   "source": [
    "dense_mat.toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of distinct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sex': 2, 'race': 5, 'native-country': 42, 'marital-status': 7, 'workclass': 9, 'education': 16, 'occupation': 15, 'relationship': 6}"
     ]
    }
   ],
   "source": [
    "len_ftrs = []\n",
    "\n",
    "for col in cols_cat:\n",
    "    (\n",
    "        len_ftrs\n",
    "        .append(\n",
    "            (col\n",
    "             , census\n",
    "                 .select(col)\n",
    "                 .distinct()\n",
    "                 .count()\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "len_ftrs = dict(len_ftrs)\n",
    "len_ftrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using hashing trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['<=50K'], [39], [2174], [0], [40], [1.0, 2.0, 1.0, 5.0], [3.0, 3.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0], [2.0, 3.0, 8.0], [0.0, 3.0, 3.0, 1.0, 4.0, 1.0, 0.0], [5.0, 5.0, 3.0], [3.0, 2.0], [4.0], [1.0, 0.0, 0.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0]], [['<=50K'], [50], [0], [0], [13], [4.0, 3.0, 1.0, 8.0], [3.0, 3.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0], [5.0, 5.0, 8.0], [0.0, 1.0, 2.0, 2.0, 8.0, 1.0, 1.0], [4.0, 2.0, 1.0], [3.0, 2.0], [4.0], [1.0, 0.0, 0.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0]], [['<=50K'], [38], [0], [0], [40], [2.0, 2.0, 0.0, 3.0], [2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0], [3.0, 2.0, 3.0], [2.0, 3.0, 1.0, 3.0, 7.0, 0.0, 1.0], [5.0, 5.0, 3.0], [3.0, 2.0], [4.0], [1.0, 0.0, 0.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0]]]"
     ]
    }
   ],
   "source": [
    "import pyspark.mllib.feature as feat\n",
    "\n",
    "final_data = (\n",
    "    census\n",
    "    .select(cols_to_keep)\n",
    "    .rdd\n",
    "    .map(lambda row: [\n",
    "        list(\n",
    "            feat.HashingTF(int(len_ftrs[col] / 2.0))\n",
    "            .transform(row[i])\n",
    "            .toArray()\n",
    "        ) if i >= 5\n",
    "        else [row[i]] \n",
    "        for i, col in enumerate(cols_to_keep)]\n",
    "    )\n",
    ")\n",
    "\n",
    "final_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 39, 2174, 0, 40, 1.0, 2.0, 1.0, 5.0, 3.0, 3.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 2.0, 3.0, 8.0, 0.0, 3.0, 3.0, 1.0, 4.0, 1.0, 0.0, 5.0, 5.0, 3.0, 3.0, 2.0, 4.0, 1.0, 0.0, 0.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0], [0, 50, 0, 0, 13, 4.0, 3.0, 1.0, 8.0, 3.0, 3.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 5.0, 5.0, 8.0, 0.0, 1.0, 2.0, 2.0, 8.0, 1.0, 1.0, 4.0, 2.0, 1.0, 3.0, 2.0, 4.0, 1.0, 0.0, 0.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0], [0, 38, 0, 0, 40, 2.0, 2.0, 0.0, 3.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 3.0, 2.0, 3.0, 2.0, 3.0, 1.0, 3.0, 7.0, 0.0, 1.0, 5.0, 5.0, 3.0, 3.0, 2.0, 4.0, 1.0, 0.0, 0.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0]]"
     ]
    }
   ],
   "source": [
    "def labelEncode(label):\n",
    "    return [int(label[0] == '>50K')]\n",
    "\n",
    "final_data = (\n",
    "    final_data\n",
    "    .map(lambda row: labelEncode(row[0]) \n",
    "         + [item \n",
    "            for sublist in row[1:] \n",
    "            for item in sublist]\n",
    "        )\n",
    ")\n",
    "\n",
    "final_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an RDD for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [39.0,2174.0,0.0,40.0,1.0,2.0,1.0,5.0,3.0,3.0,0.0,0.0,1.0,0.0,1.0,1.0,2.0,3.0,8.0,0.0,3.0,3.0,1.0,4.0,1.0,0.0,5.0,5.0,3.0,3.0,2.0,4.0,1.0,0.0,0.0,3.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,1.0,1.0,2.0,1.0,1.0,0.0]), LabeledPoint(0.0, [50.0,0.0,0.0,13.0,4.0,3.0,1.0,8.0,3.0,3.0,0.0,0.0,1.0,0.0,1.0,1.0,5.0,5.0,8.0,0.0,1.0,2.0,2.0,8.0,1.0,1.0,4.0,2.0,1.0,3.0,2.0,4.0,1.0,0.0,0.0,3.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,1.0,1.0,2.0,1.0,1.0,0.0])]"
     ]
    }
   ],
   "source": [
    "import pyspark.mllib.regression as reg\n",
    "\n",
    "final_data_income = (\n",
    "    final_data\n",
    "    .map(lambda row: reg.LabeledPoint(\n",
    "        row[0]\n",
    "        , ln.Vectors.dense(row[1:]))\n",
    "        )\n",
    ")\n",
    "final_data_income.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(40.0, [0.0,39.0,2174.0,0.0,1.0,2.0,1.0,5.0,3.0,3.0,0.0,0.0,1.0,0.0,1.0,1.0,2.0,3.0,8.0,0.0,3.0,3.0,1.0,4.0,1.0,0.0,5.0,5.0,3.0,3.0,2.0,4.0,1.0,0.0,0.0,3.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,1.0,1.0,2.0,1.0,1.0,0.0]), LabeledPoint(13.0, [0.0,50.0,0.0,0.0,4.0,3.0,1.0,8.0,3.0,3.0,0.0,0.0,1.0,0.0,1.0,1.0,5.0,5.0,8.0,0.0,1.0,2.0,2.0,8.0,1.0,1.0,4.0,2.0,1.0,3.0,2.0,4.0,1.0,0.0,0.0,3.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,1.0,1.0,2.0,1.0,1.0,0.0])]"
     ]
    }
   ],
   "source": [
    "final_data_hours = (\n",
    "    final_data\n",
    "    .map(lambda row: reg.LabeledPoint(\n",
    "        row[4]\n",
    "        , ln.Vectors.dense(row[0:4] + row[5:]))\n",
    "        )\n",
    ")\n",
    "final_data_hours.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    final_data_income_train\n",
    "    , final_data_income_test\n",
    ") = (\n",
    "    final_data_income.randomSplit([0.7, 0.3])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(\n",
    "    final_data_hours_train\n",
    "    , final_data_hours_test\n",
    ") = (\n",
    "    final_data_hours.randomSplit([0.7, 0.3])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting hours of work for census respondents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression (benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/lib/pyspark.zip/pyspark/mllib/regression.py:281: UserWarning: Deprecated in 2.0.0. Use ml.regression.LinearRegression."
     ]
    }
   ],
   "source": [
    "workhours_model_lm = reg.LinearRegressionWithSGD.train(\n",
    "    final_data_hours_train\n",
    "    , iterations = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.0 -5.75389450988e+69\n",
      "16.0 -7.41875981037e+69\n",
      "45.0 -7.88395163906e+69\n",
      "50.0 -2.16395670855e+75\n",
      "50.0 -4.98447372989e+69\n",
      "40.0 -6.19604774416e+69\n",
      "45.0 -5.39661643836e+69\n",
      "35.0 -4.15537088245e+69\n",
      "40.0 -5.0276020275e+69\n",
      "40.0 -8.55190824921e+69"
     ]
    }
   ],
   "source": [
    "small_sample_hours = sc.parallelize(final_data_hours_test.take(10))\n",
    "\n",
    "for t,p in zip(\n",
    "    small_sample_hours\n",
    "        .map(lambda row: row.label)\n",
    "        .collect()\n",
    "    , workhours_model_lm.predict(\n",
    "        small_sample_hours\n",
    "            .map(lambda row: row.features)\n",
    "    ).collect()):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting income levels of census respondents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/lib/pyspark.zip/pyspark/mllib/classification.py:313: UserWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS."
     ]
    }
   ],
   "source": [
    "import pyspark.mllib.classification as cl\n",
    "\n",
    "income_model_lr = cl.LogisticRegressionWithSGD.train(\n",
    "    final_data_income_train\n",
    "    , iterations=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1\n",
      "0.0 1\n",
      "0.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.0 1\n",
      "0.0 1\n",
      "1.0 1\n",
      "0.0 1\n",
      "0.0 1"
     ]
    }
   ],
   "source": [
    "small_sample_income = sc.parallelize(final_data_income_test.take(10))\n",
    "\n",
    "for t,p in zip(\n",
    "    small_sample_income\n",
    "        .map(lambda row: row.label)\n",
    "        .collect()\n",
    "    , income_model_lr.predict(\n",
    "        small_sample_income\n",
    "            .map(lambda row: row.features)\n",
    "    ).collect()):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5"
     ]
    }
   ],
   "source": [
    "income_model_lr.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseVector([1.7096, 269.308, -7.9421, 1.3769, 0.0074, -0.0321, 0.021, -0.0719, 0.1393, -0.0532, -0.1522, -0.04, 0.0985, 0.0468, -0.3336, 0.009, 0.689, 0.6593, -0.0671, -0.0023, -0.0763, -0.1802, -0.1886, -0.3079, -0.0625, 0.1184, -0.4948, -0.8701, -1.1213, -0.2172, -0.287, -0.7601, -0.0636, -0.0038, -0.0084, -0.192, -0.0023, -0.0618, -0.0033, -0.0015, 0.0035, -0.015, 0.0, -0.1738, -0.0313, -0.006, -0.0161, -0.072, -0.0719, -0.1462, -0.0727, -0.0875, 0.0])"
     ]
    }
   ],
   "source": [
    "income_model_lr.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "income_model_svm = cl.SVMWithSGD.train(\n",
    "    final_data_income\n",
    "    , iterations=100\n",
    "    , step=0.98\n",
    "    , miniBatchFraction=1/3.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "1.0 1\n",
      "1.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "1.0 0\n",
      "0.0 0\n",
      "0.0 0"
     ]
    }
   ],
   "source": [
    "for t,p in zip(\n",
    "    small_sample_income\n",
    "        .map(lambda row: row.label)\n",
    "        .collect()\n",
    "    , income_model_svm.predict(\n",
    "        small_sample_income\n",
    "            .map(lambda row: row.features)\n",
    "    ).collect()):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseVector([0.2073, 30.1347, 4.1533, 0.388, -0.0469, -0.1314, 0.0327, -0.4033, 0.457, -0.129, -0.4897, -0.152, 0.3058, 0.1524, -1.134, -0.0676, 1.665, 1.7398, -0.3221, -0.063, -0.1754, -0.5714, -0.6371, -0.9501, -0.2668, 0.373, -1.6258, -2.5894, -3.3322, -0.7352, -0.8754, -2.3881, -0.2096, -0.0141, -0.0265, -0.6443, -0.0026, -0.2072, -0.0107, -0.0049, 0.0078, -0.0532, 0.0, -0.5889, -0.1017, -0.0148, -0.0541, -0.2429, -0.2379, -0.4811, -0.2493, -0.2859, 0.0])"
     ]
    }
   ],
   "source": [
    "income_model_svm.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building clustering models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.mllib.clustering as clu\n",
    "\n",
    "model = clu.KMeans.train(\n",
    "    final_data.map(lambda row: row[1:])\n",
    "    , 2\n",
    "    , maxIterations=10\n",
    "    , initializationMode='random'\n",
    "    , seed=666\n",
    "    , initializationSteps=5\n",
    "    , epsilon=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0126632823359\n",
      "0.226522171727"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as m\n",
    "\n",
    "predicted = (\n",
    "    model\n",
    "        .predict(\n",
    "            final_data.map(lambda row: row[1:])\n",
    "        )\n",
    ")\n",
    "predicted = predicted.collect()\n",
    "\n",
    "true = final_data.map(lambda row: row[0]).collect()\n",
    "\n",
    "print(m.homogeneity_score(true, predicted))\n",
    "print(m.completeness_score(true, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing performance statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.mllib.evaluation as ev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Regression metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_pred_reg = (\n",
    "    final_data_hours_test\n",
    "    .map(lambda row: (\n",
    "         float(workhours_model_lm.predict(row.features))\n",
    "         , row.label))\n",
    ")\n",
    "\n",
    "metrics_lm = ev.RegressionMetrics(true_pred_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:  -8.23790361194809e+147\n",
      "Explained Variance:  1.2821818769638103e+150\n",
      "meanAbsoluteError:  1.613057724820514e+74"
     ]
    }
   ],
   "source": [
    "print('R^2: ', metrics_lm.r2)\n",
    "print('Explained Variance: ', metrics_lm.explainedVariance)\n",
    "print('meanAbsoluteError: ', metrics_lm.meanAbsoluteError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderPR:  0.5768927693066335\n",
      "areaUnderROC:  0.5768927693066335"
     ]
    }
   ],
   "source": [
    "true_pred_class_lr = (\n",
    "    final_data_income_test\n",
    "    .map(lambda row: (\n",
    "        float(income_model_lr.predict(row.features))\n",
    "        , row.label))\n",
    ")\n",
    "\n",
    "metrics_lr = ev.BinaryClassificationMetrics(true_pred_class_lr)\n",
    "\n",
    "print('areaUnderPR: ', metrics_lr.areaUnderPR)\n",
    "print('areaUnderROC: ', metrics_lr.areaUnderPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.7555487368313388"
     ]
    }
   ],
   "source": [
    "trainErr = (\n",
    "    true_pred_class_lr\n",
    "    .filter(lambda lp: lp[0] != lp[1]).count() \n",
    "    / float(true_pred_class_lr.count())\n",
    ")\n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderPR:  0.5764976815370444\n",
      "areaUnderROC:  0.5764976815370444"
     ]
    }
   ],
   "source": [
    "true_pred_class_svm = (\n",
    "    final_data_income_test\n",
    "    .map(lambda row: (\n",
    "        float(income_model_svm.predict(row.features))\n",
    "        , row.label))\n",
    ")\n",
    "\n",
    "metrics_svm = ev.BinaryClassificationMetrics(true_pred_class_svm)\n",
    "\n",
    "print('areaUnderPR: ', metrics_svm.areaUnderPR)\n",
    "print('areaUnderROC: ', metrics_svm.areaUnderPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.22634755037332516"
     ]
    }
   ],
   "source": [
    "trainErr = (\n",
    "    true_pred_class_svm\n",
    "    .filter(lambda lp: lp[0] != lp[1]).count() \n",
    "    / float(true_pred_class_svm.count())\n",
    ")\n",
    "\n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((0.0, 1.0), 1250), ((1.0, 0.0), 963), ((0.0, 0.0), 6410), ((1.0, 1.0), 1154)]"
     ]
    }
   ],
   "source": [
    "(\n",
    "    true_pred_class_svm\n",
    "    .map(lambda el: ((el), 1))\n",
    "    .reduceByKey(lambda x,y: x+y)\n",
    "    .take(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((0.0, 1.0), 249), ((1.0, 0.0), 7138), ((0.0, 0.0), 235), ((1.0, 1.0), 2155)]"
     ]
    }
   ],
   "source": [
    "(\n",
    "    true_pred_class_lr\n",
    "    .map(lambda el: ((el), 1))\n",
    "    .reduceByKey(lambda x,y: x+y)\n",
    "    .take(4)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
