{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Cookbook\n",
    "\n",
    "### Tomasz Drabas, Denny Lee\n",
    "#### Version: 0.1\n",
    "#### Date: 2/28/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "32561"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as func\n",
    "census_path = '../data/census_income.csv'\n",
    "\n",
    "census = spark.read.csv(census_path, header=True, inferSchema=True)\n",
    "\n",
    "for col, typ in census.dtypes:\n",
    "    if typ == 'string':\n",
    "        census = census.withColumn(col, func.ltrim(func.rtrim(census[col])))\n",
    "census.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+------+------------+-------------+--------------------+-----------------+-------------+------------------+------+------------+------------+--------------+--------------+-----+\n",
      "|age|       workclass|fnlwgt|   education|education-num|      marital-status|       occupation| relationship|              race|   sex|capital-gain|capital-loss|hours-per-week|native-country|label|\n",
      "+---+----------------+------+------------+-------------+--------------------+-----------------+-------------+------------------+------+------------+------------+--------------+--------------+-----+\n",
      "| 39|       State-gov| 77516|   Bachelors|           13|       Never-married|     Adm-clerical|Not-in-family|             White|  Male|        2174|           0|            40| United-States|<=50K|\n",
      "| 50|Self-emp-not-inc| 83311|   Bachelors|           13|  Married-civ-spouse|  Exec-managerial|      Husband|             White|  Male|           0|           0|            13| United-States|<=50K|\n",
      "| 38|         Private|215646|     HS-grad|            9|            Divorced|Handlers-cleaners|Not-in-family|             White|  Male|           0|           0|            40| United-States|<=50K|\n",
      "| 53|         Private|234721|        11th|            7|  Married-civ-spouse|Handlers-cleaners|      Husband|             Black|  Male|           0|           0|            40| United-States|<=50K|\n",
      "| 28|         Private|338409|   Bachelors|           13|  Married-civ-spouse|   Prof-specialty|         Wife|             Black|Female|           0|           0|            40|          Cuba|<=50K|\n",
      "| 37|         Private|284582|     Masters|           14|  Married-civ-spouse|  Exec-managerial|         Wife|             White|Female|           0|           0|            40| United-States|<=50K|\n",
      "| 49|         Private|160187|         9th|            5|Married-spouse-ab...|    Other-service|Not-in-family|             Black|Female|           0|           0|            16|       Jamaica|<=50K|\n",
      "| 52|Self-emp-not-inc|209642|     HS-grad|            9|  Married-civ-spouse|  Exec-managerial|      Husband|             White|  Male|           0|           0|            45| United-States| >50K|\n",
      "| 31|         Private| 45781|     Masters|           14|       Never-married|   Prof-specialty|Not-in-family|             White|Female|       14084|           0|            50| United-States| >50K|\n",
      "| 42|         Private|159449|   Bachelors|           13|  Married-civ-spouse|  Exec-managerial|      Husband|             White|  Male|        5178|           0|            40| United-States| >50K|\n",
      "| 37|         Private|280464|Some-college|           10|  Married-civ-spouse|  Exec-managerial|      Husband|             Black|  Male|           0|           0|            80| United-States| >50K|\n",
      "| 30|       State-gov|141297|   Bachelors|           13|  Married-civ-spouse|   Prof-specialty|      Husband|Asian-Pac-Islander|  Male|           0|           0|            40|         India| >50K|\n",
      "| 23|         Private|122272|   Bachelors|           13|       Never-married|     Adm-clerical|    Own-child|             White|Female|           0|           0|            30| United-States|<=50K|\n",
      "| 32|         Private|205019|  Assoc-acdm|           12|       Never-married|            Sales|Not-in-family|             Black|  Male|           0|           0|            50| United-States|<=50K|\n",
      "| 40|         Private|121772|   Assoc-voc|           11|  Married-civ-spouse|     Craft-repair|      Husband|Asian-Pac-Islander|  Male|           0|           0|            40|             ?| >50K|\n",
      "| 34|         Private|245487|     7th-8th|            4|  Married-civ-spouse| Transport-moving|      Husband|Amer-Indian-Eskimo|  Male|           0|           0|            45|        Mexico|<=50K|\n",
      "| 25|Self-emp-not-inc|176756|     HS-grad|            9|       Never-married|  Farming-fishing|    Own-child|             White|  Male|           0|           0|            35| United-States|<=50K|\n",
      "| 32|         Private|186824|     HS-grad|            9|       Never-married|Machine-op-inspct|    Unmarried|             White|  Male|           0|           0|            40| United-States|<=50K|\n",
      "| 38|         Private| 28887|        11th|            7|  Married-civ-spouse|            Sales|      Husband|             White|  Male|           0|           0|            50| United-States|<=50K|\n",
      "| 43|Self-emp-not-inc|292175|     Masters|           14|            Divorced|  Exec-managerial|    Unmarried|             White|Female|           0|           0|            45| United-States| >50K|\n",
      "+---+----------------+------+------------+-------------+--------------------+-----------------+-------------+------------------+------+------------+------------+--------------+--------------+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "census.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: integer (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education-num: integer (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital-gain: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- hours-per-week: integer (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- label: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "census.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading into RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['39', 'State-gov', '77516', 'Bachelors', '13', 'Never-married', 'Adm-clerical', 'Not-in-family', 'White', 'Male', '2174', '0', '40', 'United-States', '<=50K']]"
     ]
    }
   ],
   "source": [
    "census_rdd = sc.textFile(census_path)\n",
    "\n",
    "header = census_rdd.first().split(',')\n",
    "census_split = (\n",
    "    census_rdd\n",
    "    .map(lambda row: row.split(','))\n",
    "    .map(lambda row: [e.strip() for e in row])\n",
    "    .filter(lambda row: row != header) # remove header\n",
    ")\n",
    "\n",
    "census_split.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of columns to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', 'age', 'capital-gain', 'capital-loss', 'hours-per-week', 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']"
     ]
    }
   ],
   "source": [
    "cols_to_keep = census.dtypes\n",
    "\n",
    "cols_to_keep = (\n",
    "    ['label','age', 'capital-gain', 'capital-loss','hours-per-week'] + \n",
    "    [e[0] for e in cols_to_keep[:-1] if e[1] == 'string']\n",
    ")\n",
    "\n",
    "cols_to_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get numeric and categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['age', 'capital-gain', 'capital-loss', 'hours-per-week'], ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country'])"
     ]
    }
   ],
   "source": [
    "import pyspark.mllib.stat as st\n",
    "import numpy as np\n",
    "\n",
    "census_subset = census.select(cols_to_keep)\n",
    "\n",
    "cols_num = [e[0] for e in census_subset.dtypes if e[1] == 'int']\n",
    "cols_cat = [e[0] for e in census_subset.dtypes[1:] if e[1] == 'string']\n",
    "cols_num, cols_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age: min->17.0, mean->38.6, max->90.0, stdev->13.6\n",
      "capital-gain: min->0.0, mean->1077.6, max->99999.0, stdev->7385.3\n",
      "capital-loss: min->0.0, mean->87.3, max->4356.0, stdev->403.0\n",
      "hours-per-week: min->1.0, mean->40.4, max->99.0, stdev->12.3"
     ]
    }
   ],
   "source": [
    "rdd_num = census_subset.select(cols_num).rdd.map(lambda row: [e for e in row])\n",
    "stats_num = st.Statistics.colStats(rdd_num)\n",
    "\n",
    "for col, min_, mean_, max_, var_ in zip(\n",
    "      cols_num\n",
    "    , stats_num.min()\n",
    "    , stats_num.mean()\n",
    "    , stats_num.max()\n",
    "    , stats_num.variance()\n",
    "):\n",
    "    print('{0}: min->{1:.1f}, mean->{2:.1f}, max->{3:.1f}, stdev->{4:.1f}'\n",
    "          .format(col, min_, mean_, max_, np.sqrt(var_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex [('Male', 21790), ('Female', 10771)] \n",
      "\n",
      "race [('White', 27816), ('Black', 3124), ('Asian-Pac-Islander', 1039), ('Amer-Indian-Eskimo', 311), ('Other', 271)] \n",
      "\n",
      "label [('<=50K', 24720), ('>50K', 7841)] \n",
      "\n",
      "native-country [('United-States', 29170), ('Mexico', 643), ('?', 583), ('Philippines', 198), ('Germany', 137), ('Canada', 121), ('Puerto-Rico', 114), ('El-Salvador', 106), ('India', 100), ('Cuba', 95), ('England', 90), ('Jamaica', 81), ('South', 80), ('China', 75), ('Italy', 73), ('Dominican-Republic', 70), ('Vietnam', 67), ('Guatemala', 64), ('Japan', 62), ('Poland', 60), ('Columbia', 59), ('Taiwan', 51), ('Haiti', 44), ('Iran', 43), ('Portugal', 37), ('Nicaragua', 34), ('Peru', 31), ('France', 29), ('Greece', 29), ('Ecuador', 28), ('Ireland', 24), ('Hong', 20), ('Trinadad&Tobago', 19), ('Cambodia', 19), ('Laos', 18), ('Thailand', 18), ('Yugoslavia', 16), ('Outlying-US(Guam-USVI-etc)', 14), ('Hungary', 13), ('Honduras', 13), ('Scotland', 12), ('Holand-Netherlands', 1)] \n",
      "\n",
      "marital-status [('Married-civ-spouse', 14976), ('Never-married', 10683), ('Divorced', 4443), ('Separated', 1025), ('Widowed', 993), ('Married-spouse-absent', 418), ('Married-AF-spouse', 23)] \n",
      "\n",
      "workclass [('Private', 22696), ('Self-emp-not-inc', 2541), ('Local-gov', 2093), ('?', 1836), ('State-gov', 1298), ('Self-emp-inc', 1116), ('Federal-gov', 960), ('Without-pay', 14), ('Never-worked', 7)] \n",
      "\n",
      "education [('HS-grad', 10501), ('Some-college', 7291), ('Bachelors', 5355), ('Masters', 1723), ('Assoc-voc', 1382), ('11th', 1175), ('Assoc-acdm', 1067), ('10th', 933), ('7th-8th', 646), ('Prof-school', 576), ('9th', 514), ('12th', 433), ('Doctorate', 413), ('5th-6th', 333), ('1st-4th', 168), ('Preschool', 51)] \n",
      "\n",
      "occupation [('Prof-specialty', 4140), ('Craft-repair', 4099), ('Exec-managerial', 4066), ('Adm-clerical', 3770), ('Sales', 3650), ('Other-service', 3295), ('Machine-op-inspct', 2002), ('?', 1843), ('Transport-moving', 1597), ('Handlers-cleaners', 1370), ('Farming-fishing', 994), ('Tech-support', 928), ('Protective-serv', 649), ('Priv-house-serv', 149), ('Armed-Forces', 9)] \n",
      "\n",
      "relationship [('Husband', 13193), ('Not-in-family', 8305), ('Own-child', 5068), ('Unmarried', 3446), ('Wife', 1568), ('Other-relative', 981)]"
     ]
    }
   ],
   "source": [
    "rdd_cat = census_subset.select(cols_cat + ['label']).rdd.map(lambda row: [e for e in row])\n",
    "\n",
    "results_cat = {}\n",
    "\n",
    "for i, col in enumerate(cols_cat + ['label']):\n",
    "    results_cat[col] = rdd_cat.groupBy(lambda row: row[i]).map(lambda el: (el[0], len(el[1]))).collect()\n",
    "\n",
    "for k in results_cat:\n",
    "    print(k, sorted(results_cat[k], key=lambda el: el[1], reverse=True), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correlations = st.Statistics.corr(rdd_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "     capital-gain 0.077674498166\n",
      "     capital-loss 0.057774539479\n",
      "     hours-per-week 0.0687557075095\n",
      "\n",
      "capital-gain\n",
      "     age 0.077674498166\n",
      "     hours-per-week 0.0784086153901\n",
      "\n",
      "capital-loss\n",
      "     age 0.057774539479\n",
      "     hours-per-week 0.0542563622727\n",
      "\n",
      "hours-per-week\n",
      "     age 0.0687557075095\n",
      "     capital-gain 0.0784086153901\n",
      "     capital-loss 0.0542563622727"
     ]
    }
   ],
   "source": [
    "for i, el_i in enumerate(abs(correlations) > 0.05):\n",
    "    print(cols_num[i])\n",
    "    \n",
    "    for j, el_j in enumerate(el_i):\n",
    "        if el_j and j != i:\n",
    "            print('    ', cols_num[j], correlations[i][j])\n",
    "            \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0"
     ]
    }
   ],
   "source": [
    "import pyspark.mllib.linalg as ln\n",
    "\n",
    "census_occupation = census.groupby('label').pivot('occupation').count()\n",
    "\n",
    "census_occupation_coll = (\n",
    "    census_occupation\n",
    "    .rdd\n",
    "    .map(lambda row: (row[1:]))\n",
    "    .flatMap(lambda row: row)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "len_row = len(census_occupation.collect()[0]) - 1\n",
    "dense_mat = ln.Matrices.dense(len_row, 2, census_occupation_coll)\n",
    "\n",
    "chi_sq = st.Statistics.chiSqTest(dense_mat)\n",
    "\n",
    "print(chi_sq.pValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of distinct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_ftrs = []\n",
    "\n",
    "for col in cols_to_keep[5:]:\n",
    "    len_ftrs.append((col, census.select(col).distinct().count()))\n",
    "    \n",
    "len_ftrs = dict(len_ftrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using hashing trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['<=50K'], [39], [2174], [0], [40], [1.0, 2.0, 1.0, 5.0], [3.0, 3.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0], [2.0, 3.0, 8.0], [0.0, 3.0, 3.0, 1.0, 4.0, 1.0, 0.0], [5.0, 5.0, 3.0], [3.0, 2.0], [4.0], [1.0, 0.0, 0.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0]], [['<=50K'], [50], [0], [0], [13], [4.0, 3.0, 1.0, 8.0], [3.0, 3.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0], [5.0, 5.0, 8.0], [0.0, 1.0, 2.0, 2.0, 8.0, 1.0, 1.0], [4.0, 2.0, 1.0], [3.0, 2.0], [4.0], [1.0, 0.0, 0.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0]], [['<=50K'], [38], [0], [0], [40], [2.0, 2.0, 0.0, 3.0], [2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0], [3.0, 2.0, 3.0], [2.0, 3.0, 1.0, 3.0, 7.0, 0.0, 1.0], [5.0, 5.0, 3.0], [3.0, 2.0], [4.0], [1.0, 0.0, 0.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0]]]"
     ]
    }
   ],
   "source": [
    "import pyspark.mllib.feature as feat\n",
    "\n",
    "final_data = (\n",
    "    census\n",
    "    .select(cols_to_keep)\n",
    "    .rdd\n",
    "    .map(lambda row: [\n",
    "        list(\n",
    "            feat.HashingTF(int(len_ftrs[col] / 2.0))\n",
    "            .transform(row[i])\n",
    "            .toArray()\n",
    "        ) if i > 4\n",
    "        else [row[i]] \n",
    "        for i, col in enumerate(cols_to_keep)]\n",
    "    )\n",
    ")\n",
    "\n",
    "final_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labelEncode(label):\n",
    "    return [int(label[0] == '>50K')]\n",
    "\n",
    "final_data = final_data.map(lambda row: labelEncode(row[0]) + [item for sublist in row[1:]\n",
    "                  for item in sublist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an RDD for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [39.0,2174.0,0.0,40.0,1.0,2.0,1.0,5.0,3.0,3.0,0.0,0.0,1.0,0.0,1.0,1.0,2.0,3.0,8.0,0.0,3.0,3.0,1.0,4.0,1.0,0.0,5.0,5.0,3.0,3.0,2.0,4.0,1.0,0.0,0.0,3.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,1.0,1.0,2.0,1.0,1.0,0.0]), LabeledPoint(0.0, [50.0,0.0,0.0,13.0,4.0,3.0,1.0,8.0,3.0,3.0,0.0,0.0,1.0,0.0,1.0,1.0,5.0,5.0,8.0,0.0,1.0,2.0,2.0,8.0,1.0,1.0,4.0,2.0,1.0,3.0,2.0,4.0,1.0,0.0,0.0,3.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,1.0,1.0,2.0,1.0,1.0,0.0])]"
     ]
    }
   ],
   "source": [
    "import pyspark.mllib.feature as ft\n",
    "import pyspark.mllib.linalg as ln\n",
    "import pyspark.mllib.regression as reg\n",
    "\n",
    "final_data_income = final_data.map(lambda row: reg.LabeledPoint(row[0], ln.Vectors.dense(row[1:])))\n",
    "final_data_income.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(40.0, [0.0,39.0,2174.0,0.0,1.0,2.0,1.0,5.0,3.0,3.0,0.0,0.0,1.0,0.0,1.0,1.0,2.0,3.0,8.0,0.0,3.0,3.0,1.0,4.0,1.0,0.0,5.0,5.0,3.0,3.0,2.0,4.0,1.0,0.0,0.0,3.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,1.0,1.0,2.0,1.0,1.0,0.0]), LabeledPoint(13.0, [0.0,50.0,0.0,0.0,4.0,3.0,1.0,8.0,3.0,3.0,0.0,0.0,1.0,0.0,1.0,1.0,5.0,5.0,8.0,0.0,1.0,2.0,2.0,8.0,1.0,1.0,4.0,2.0,1.0,3.0,2.0,4.0,1.0,0.0,0.0,3.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,1.0,1.0,2.0,1.0,1.0,0.0])]"
     ]
    }
   ],
   "source": [
    "final_data_hours = final_data.map(lambda row: reg.LabeledPoint(row[4], ln.Vectors.dense(row[0:4] + row[5:])))\n",
    "final_data_hours.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting hours of work for census respondents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_reg=sc.parallelize(final_data_hours.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression (benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "workhours_model_lm = reg.LinearRegressionWithSGD.train(\n",
    "    final_data_hours\n",
    "    , iterations = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.0 -2.9887777944e+74\n",
      "13.0 -6.87179051853e+69\n",
      "40.0 -5.20607963039e+69\n",
      "40.0 -7.11940017011e+69\n",
      "40.0 -4.01270942523e+69\n",
      "40.0 -5.18608400327e+69\n",
      "16.0 -6.68844518484e+69\n",
      "45.0 -7.1064025535e+69\n",
      "50.0 -1.93621350264e+75\n",
      "40.0 -7.11855484288e+74"
     ]
    }
   ],
   "source": [
    "for t,p in zip(test_data_reg.map(lambda row: row.label).collect()\n",
    "    , workhours_model_lm.predict(test_data_reg.map(lambda row: row.features)).collect()):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting income levels of census respondents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_class=sc.parallelize(final_data_income.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.mllib.classification as cl\n",
    "\n",
    "income_model_lr = cl.LogisticRegressionWithSGD.train(\n",
    "    final_data_income\n",
    "    , iterations=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1\n",
      "0.0 1\n",
      "0.0 1\n",
      "0.0 1\n",
      "0.0 1\n",
      "0.0 1\n",
      "0.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1"
     ]
    }
   ],
   "source": [
    "for t,p in zip(test_data_class.map(lambda row: row.label).collect()\n",
    "    , income_model_lr.predict(test_data_class.map(lambda row: row.features)).collect()):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5"
     ]
    }
   ],
   "source": [
    "income_model_lr.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseVector([1.6394, 303.7119, -7.1005, 1.384, 0.014, -0.0307, 0.0211, -0.0784, 0.1424, -0.0448, -0.1531, -0.0394, 0.0919, 0.0499, -0.331, -0.0099, 0.6931, 0.6594, -0.0666, -0.0079, -0.0741, -0.1695, -0.1866, -0.2795, -0.0557, 0.1218, -0.4953, -0.871, -1.113, -0.2151, -0.2842, -0.7633, -0.0624, -0.0036, -0.0076, -0.1912, -0.0009, -0.0617, -0.0032, -0.0018, 0.0027, -0.0138, 0.0, -0.1718, -0.0297, -0.0045, -0.0158, -0.0708, -0.0704, -0.142, -0.0733, -0.0848, 0.0])"
     ]
    }
   ],
   "source": [
    "income_model_lr.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "income_model_svm = cl.SVMWithSGD.train(\n",
    "    final_data_income\n",
    "    , iterations=100\n",
    "    , step=0.98\n",
    "    , miniBatchFraction=1/3.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "1.0 0\n",
      "1.0 1\n",
      "1.0 1"
     ]
    }
   ],
   "source": [
    "for t,p in zip(test_data_class.map(lambda row: row.label).collect()\n",
    "    , income_model_svm.predict(test_data_class.map(lambda row: row.features)).collect()):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseVector([-3.7051, 75.158, 4.7048, -3.7744, -0.3049, -0.3383, -0.0043, -0.6313, -0.0741, -0.3845, -0.2785, -0.1044, 0.0738, 0.0609, -0.7295, -0.2406, 0.5397, 0.5315, -1.0844, -0.1711, -0.233, -0.4848, -0.6583, -1.1386, -0.2757, 0.1375, -1.3491, -1.7985, -2.1637, -0.7888, -0.814, -1.9143, -0.2324, -0.0074, -0.0149, -0.7041, -0.0034, -0.23, -0.0063, -0.0046, 0.0032, -0.0296, 0.0, -0.5619, -0.056, -0.0111, -0.0291, -0.252, -0.247, -0.4991, -0.2584, -0.2774, 0.0])"
     ]
    }
   ],
   "source": [
    "income_model_svm.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building clustering models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.mllib.clustering as clu\n",
    "\n",
    "model = clu.KMeans.train(\n",
    "    final_data.map(lambda row: row[1:])\n",
    "    , 2\n",
    "    , maxIterations=10\n",
    "    , initializationMode='random'\n",
    "    , seed=666\n",
    "    , initializationSteps=5\n",
    "    , epsilon=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  3.85434850e+01,   5.92231436e+02,   8.77322388e+01,\n",
      "         4.03915190e+01,   2.05295969e-01,   0.00000000e+00,\n",
      "         3.56181717e-01,   2.18628480e-01,   1.50756126e+00,\n",
      "         1.96318129e+00,   1.18974137e+00,   1.46497130e+00,\n",
      "         9.49663601e-01,   9.96543423e-01,   1.64002222e-01,\n",
      "         1.55576816e-01,   1.02771434e-02,   5.69933955e-01,\n",
      "         5.26202086e-02,   3.41738164e-01,   1.56823653e+00,\n",
      "         1.20140732e+00,   2.09039565e+00,   2.87327943e-02,\n",
      "         2.42361583e-01,   0.00000000e+00,   8.57971730e-02,\n",
      "         9.04234307e-01,   1.79309919e-02,   1.89408061e+00,\n",
      "         1.04348497e+00,   2.07447071e+00,   2.72251713e+00,\n",
      "         4.57169311e+00,   2.06564410e+00,   3.06153941e-02,\n",
      "         2.02055429e-01,   1.05641627e+00,   1.11875810e-01,\n",
      "         1.70643787e+00,   1.31069070e+00,   1.32584408e-01,\n",
      "         7.11807913e-01,   2.76001481e-01,   1.60277143e+00,\n",
      "         6.23480032e-01,   0.00000000e+00,   1.11906672e+00,\n",
      "         0.00000000e+00,   2.34800321e-01,   3.11304858e+00,\n",
      "         1.76739090e+00,   1.13165854e+00,   6.64033084e-01,\n",
      "         2.05333004e+00,   1.69795074e+00,   1.81254244e+00,\n",
      "         8.30195667e-03,   3.99172891e-01,   1.27955065e-01,\n",
      "         2.04706500e+00,   2.95512623e+00,   1.00000000e+00,\n",
      "         3.66347756e+00,   9.04512067e-01,   0.00000000e+00,\n",
      "         0.00000000e+00,   2.70406148e+00,   1.14190482e-02,\n",
      "         8.96734769e-01,   5.64779952e-03,   3.51830134e-03,\n",
      "         0.00000000e+00,   2.09863589e-02,   0.00000000e+00,\n",
      "         9.83704710e-01,   0.00000000e+00,   1.63570150e-02,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         1.83732486e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   2.12949818e-03,   8.05505833e-03,\n",
      "         1.57397692e-02,   1.13573236e-02,   4.25899636e-03,\n",
      "         0.00000000e+00,   1.54311462e-03,   7.31436331e-03,\n",
      "         7.74643541e-03,   2.21591260e-02,   0.00000000e+00,\n",
      "         9.58150731e-01,   4.45960126e-02,   7.49953707e-03,\n",
      "         1.98135918e-02,   9.44447874e-01,   9.13647306e-01,\n",
      "         1.61718412e-02,   9.61823344e-01,   9.64045429e-01,\n",
      "         0.00000000e+00]), array([  4.63584906e+01,   9.99990000e+04,   0.00000000e+00,\n",
      "         4.97987421e+01,   4.52830189e-01,   0.00000000e+00,\n",
      "         8.67924528e-01,   5.97484277e-01,   1.44025157e+00,\n",
      "         2.32704403e+00,   1.00628931e+00,   1.70440252e+00,\n",
      "         1.33962264e+00,   1.03144654e+00,   2.57861635e-01,\n",
      "         1.88679245e-02,   0.00000000e+00,   7.35849057e-01,\n",
      "         1.13207547e-01,   5.66037736e-01,   2.37735849e+00,\n",
      "         1.60377358e+00,   1.54716981e+00,   1.25786164e-02,\n",
      "         2.83018868e-01,   0.00000000e+00,   1.25786164e-02,\n",
      "         3.83647799e-01,   2.89308176e-01,   2.66037736e+00,\n",
      "         1.01257862e+00,   2.66037736e+00,   2.87421384e+00,\n",
      "         4.82389937e+00,   2.72955975e+00,   6.28930818e-03,\n",
      "         2.76729560e-01,   8.67924528e-01,   1.57232704e-01,\n",
      "         1.38993711e+00,   8.67924528e-01,   1.25786164e-02,\n",
      "         9.37106918e-01,   8.49056604e-01,   1.75471698e+00,\n",
      "         6.28930818e-01,   0.00000000e+00,   1.11320755e+00,\n",
      "         0.00000000e+00,   2.70440252e-01,   3.25157233e+00,\n",
      "         1.83647799e+00,   1.03773585e+00,   1.44654088e-01,\n",
      "         1.96855346e+00,   1.27672956e+00,   1.39622642e+00,\n",
      "         1.25786164e-02,   4.02515723e-01,   1.06918239e-01,\n",
      "         2.14465409e+00,   3.15094340e+00,   1.00000000e+00,\n",
      "         3.27672956e+00,   8.99371069e-01,   0.00000000e+00,\n",
      "         0.00000000e+00,   2.68553459e+00,   1.25786164e-02,\n",
      "         8.93081761e-01,   6.28930818e-03,   0.00000000e+00,\n",
      "         0.00000000e+00,   1.25786164e-02,   0.00000000e+00,\n",
      "         9.62264151e-01,   0.00000000e+00,   6.28930818e-03,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         1.80503145e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   6.28930818e-03,   6.28930818e-03,\n",
      "         6.28930818e-03,   6.28930818e-03,   0.00000000e+00,\n",
      "         0.00000000e+00,   6.28930818e-03,   0.00000000e+00,\n",
      "         1.88679245e-02,   5.03144654e-02,   0.00000000e+00,\n",
      "         9.18238994e-01,   1.88679245e-02,   0.00000000e+00,\n",
      "         6.28930818e-03,   9.30817610e-01,   8.99371069e-01,\n",
      "         2.51572327e-02,   9.62264151e-01,   9.62264151e-01,\n",
      "         0.00000000e+00])]"
     ]
    }
   ],
   "source": [
    "model.clusterCenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0126632823359\n",
      "0.226522171727"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as m\n",
    "\n",
    "predicted = model.predict(final_data.map(lambda row: row[1:]))\n",
    "predicted = predicted.collect()\n",
    "\n",
    "true = final_data.map(lambda row: row[0]).collect()\n",
    "\n",
    "print(m.homogeneity_score(true, predicted))\n",
    "print(m.completeness_score(true, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing performance statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.mllib.evaluation as ev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Regression metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_reg = final_data_hours.map(lambda row: row.label).zipWithIndex().map(lambda row: (row[1], row[0]))\n",
    "pred_reg = workhours_model_lm.predict(final_data_hours.map(lambda row: row.features)).zipWithIndex().map(lambda row: (row[1], float(row[0])))\n",
    "\n",
    "true_pred_reg = pred_reg.join(true_reg).map(lambda el: el[1])\n",
    "\n",
    "metrics_lm = ev.RegressionMetrics(true_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:  -3.157194770951297\n",
      "Explained Variance:  0.5521871925143494\n",
      "meanAbsoluteError:  0.7600196554159885"
     ]
    }
   ],
   "source": [
    "print('R^2: ', metrics_lm.r2)\n",
    "print('Explained Variance: ', metrics_lm.explainedVariance)\n",
    "print('meanAbsoluteError: ', metrics_lm.meanAbsoluteError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pred_class_lr = final_data_income.map(lambda row: (float(income_model_lr.predict(row.features)), row.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics_lr = ev.BinaryClassificationMetrics(true_pred_class_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderPR:  0.5764236038031435\n",
      "areaUnderROC:  0.5764236038031435"
     ]
    }
   ],
   "source": [
    "print('areaUnderPR: ', metrics_lr.areaUnderPR)\n",
    "print('areaUnderROC: ', metrics_lr.areaUnderPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.7600196554159885"
     ]
    }
   ],
   "source": [
    "trainErr = true_pred_class_lr.filter(lambda lp: lp[0] != lp[1]).count() / float(true_pred_class_lr.count())\n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderPR:  0.5741069899450867\n",
      "areaUnderROC:  0.5741069899450867"
     ]
    }
   ],
   "source": [
    "true_pred_class_svm = final_data_income.map(lambda row: (float(income_model_svm.predict(row.features)), row.label))\n",
    "\n",
    "metrics_svm = ev.BinaryClassificationMetrics(true_pred_class_svm)\n",
    "\n",
    "print('areaUnderPR: ', metrics_svm.areaUnderPR)\n",
    "print('areaUnderROC: ', metrics_svm.areaUnderPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.22112342987008998"
     ]
    }
   ],
   "source": [
    "trainErr = true_pred_class_svm.filter(lambda lp: lp[0] != lp[1]).count() / float(true_pred_class_svm.count())\n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((0.0, 1.0), 4120), ((1.0, 0.0), 3080), ((0.0, 0.0), 21640), ((1.0, 1.0), 3721)]"
     ]
    }
   ],
   "source": [
    "true_pred_class_svm.map(lambda el: ((el), 1)).reduceByKey(lambda x,y: x+y).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((0.0, 1.0), 773), ((1.0, 0.0), 23974), ((0.0, 0.0), 746), ((1.0, 1.0), 7068)]"
     ]
    }
   ],
   "source": [
    "true_pred_class_lr.map(lambda el: ((el), 1)).reduceByKey(lambda x,y: x+y).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
